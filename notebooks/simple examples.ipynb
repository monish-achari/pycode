{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "macro-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"name\":\"Abhi\", \"age\":24, \"city\":\"Bidar\"},\n",
    "    {\"name\":\"Minu\", \"age\":27, \"city\":\"Gulbarga\"},\n",
    "    {\"name\":\"Anji\", \"age\":22, \"city\":\"Bidar\"},\n",
    "    {\"name\":\"Rama\", \"age\":21, \"city\":\"Raichur\"},\n",
    "    {\"name\":\"Rakshita\", \"age\":21, \"city\":\"Bangalore\"},\n",
    "    {\"name\":\"Ramya\", \"age\":23, \"city\":\"Bangalore\"},\n",
    "    {\"name\":\"Ashwini\", \"age\":26, \"city\":\"Mysore\"},\n",
    "    {\"name\":\"Ravi\", \"age\":28, \"city\":\"Mangalore\"},\n",
    "    {\"name\":\"Krish\", \"age\":34, \"city\":\"Bangalore\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alpha-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "muslim-wrapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/31 15:32:00 WARN Utils: Your hostname, Aspire resolves to a loopback address: 127.0.1.1; using 192.168.1.24 instead (on interface wlan0)\n",
      "21/12/31 15:32:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/moni/.local/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/31 15:32:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "revised-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hawaiian-elevation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+\n",
      "|age|     city|    name|\n",
      "+---+---------+--------+\n",
      "| 24|    Bidar|    Abhi|\n",
      "| 27| Gulbarga|    Minu|\n",
      "| 22|    Bidar|    Anji|\n",
      "| 21|  Raichur|    Rama|\n",
      "| 21|Bangalore|Rakshita|\n",
      "| 23|Bangalore|   Ramya|\n",
      "| 26|   Mysore| Ashwini|\n",
      "| 28|Mangalore|    Ravi|\n",
      "| 34|Bangalore|   Krish|\n",
      "+---+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "appreciated-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "polish-video",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     city|count|\n",
      "+---------+-----+\n",
      "| Gulbarga|    1|\n",
      "|    Bidar|    2|\n",
      "|  Raichur|    1|\n",
      "|Bangalore|    3|\n",
      "|   Mysore|    1|\n",
      "|Mangalore|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"city\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "greenhouse-spelling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 27|    1|\n",
      "| 24|    1|\n",
      "| 22|    1|\n",
      "| 21|    2|\n",
      "| 23|    1|\n",
      "| 26|    1|\n",
      "| 34|    1|\n",
      "| 28|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "incorrect-preservation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Abhi', 'age': 24, 'city': 'Bidar'},\n",
       " {'name': 'Minu', 'age': 27, 'city': 'Gulbarga'},\n",
       " {'name': 'Anji', 'age': 22, 'city': 'Bidar'},\n",
       " {'name': 'Rama', 'age': 21, 'city': 'Raichur'},\n",
       " {'name': 'Rakshita', 'age': 21, 'city': 'Bangalore'},\n",
       " {'name': 'Ramya', 'age': 23, 'city': 'Bangalore'},\n",
       " {'name': 'Ashwini', 'age': 26, 'city': 'Mysore'},\n",
       " {'name': 'Ravi', 'age': 28, 'city': 'Mangalore'},\n",
       " {'name': 'Krish', 'age': 34, 'city': 'Bangalore'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "pretty-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.util import itertools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "attended-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city(item):\n",
    "    return item[\"city\"]\n",
    "\n",
    "dd = itertools.groupby(data,get_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "serial-separate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidar 1\n",
      "Gulbarga 1\n",
      "Bidar 1\n",
      "Raichur 1\n",
      "Bangalore 2\n",
      "Mysore 1\n",
      "Mangalore 1\n",
      "Bangalore 1\n"
     ]
    }
   ],
   "source": [
    "for i,j in dd:\n",
    "    print(i,len(list(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "spatial-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "exclusive-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city(item):\n",
    "    return item[\"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fiscal-tourist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Abhi', 'age': 24, 'city': 'Bidar'},\n",
       " {'name': 'Minu', 'age': 27, 'city': 'Gulbarga'},\n",
       " {'name': 'Anji', 'age': 22, 'city': 'Bidar'},\n",
       " {'name': 'Rama', 'age': 21, 'city': 'Raichur'},\n",
       " {'name': 'Rakshita', 'age': 21, 'city': 'Bangalore'},\n",
       " {'name': 'Ramya', 'age': 23, 'city': 'Bangalore'},\n",
       " {'name': 'Ashwini', 'age': 26, 'city': 'Mysore'},\n",
       " {'name': 'Ravi', 'age': 28, 'city': 'Mangalore'},\n",
       " {'name': 'Krish', 'age': 34, 'city': 'Bangalore'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "computational-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = sorted(data,key=operator.itemgetter(\"age\"))\n",
    "\n",
    "ddf = itertools.groupby(sorted_data,get_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "twenty-hydrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 2\n",
      "22 1\n",
      "23 1\n",
      "24 1\n",
      "26 1\n",
      "27 1\n",
      "28 1\n",
      "34 1\n"
     ]
    }
   ],
   "source": [
    "for i,j in ddf:\n",
    "    print(i,len(list(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "suffering-facial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Column', 'DataType', 'PythonEvalType', 'SparkContext', 'StringType', 'StructType', 'UDFRegistration', 'UserDefinedFunction', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_create_udf', '_parse_datatype_string', '_prepare_for_python_RDD', '_test', '_to_java_column', '_to_seq', '_wrap_function', 'functools', 'sys', 'to_arrow_type']\n"
     ]
    }
   ],
   "source": [
    "print(dir(pyspark.sql.udf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-container",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
